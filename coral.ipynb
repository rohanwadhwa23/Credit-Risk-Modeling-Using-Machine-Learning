{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ybfev1A8wxS8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JahQmtn3wz2G"
   },
   "outputs": [],
   "source": [
    "def default_function(row):\n",
    "    ## returning 0 for firms with no defaults\n",
    "    if pd.isnull(row['def_date']):\n",
    "        return 0\n",
    "    \n",
    "    ## checking for a default within 12 months of April of next firm year;\n",
    "    ## as discussed in the 'When does a firm year end' sidebar,\n",
    "    ## financial statement data for a given year is not actually available until ~March/April of the next year\n",
    "    diff_default = row['def_date'] - row['stmt_date']\n",
    "    \n",
    "    if diff_default <= datetime.timedelta(days=486) and diff_default > datetime.timedelta(days=120):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def reduce_ratio_dimentionality(df, ratio_type, new, preproc_params):\n",
    "  if new:\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(df)\n",
    "    preproc_params[\"pca\"+ratio_type] = pca\n",
    "    result = pca.transform(df)\n",
    "  else:\n",
    "    pca = preproc_params[\"pca\"+ratio_type]\n",
    "    result = pca.transform(df)\n",
    "  return result, preproc_params\n",
    "\n",
    "\n",
    "def preprocessor(df, preproc_params = {}, data_path=\"./\", new = True):\n",
    "    cleaned_data = df.copy()\n",
    "    \n",
    "    ## dropping column with no non-null values\n",
    "    cleaned_data = cleaned_data.drop(columns='eqty_corp_family_tot')\n",
    "\n",
    "    date_cols = ['stmt_date', 'def_date']\n",
    "\n",
    "    cleaned_data[date_cols[0]] = pd.to_datetime(cleaned_data[date_cols[0]], format='%Y-%m-%d')\n",
    "    cleaned_data[date_cols[1]] = pd.to_datetime(cleaned_data[date_cols[1]], format='%d/%m/%Y')\n",
    "\n",
    "    ## filling in NAs for margin_fin and roe by using definition\n",
    "\n",
    "    cleaned_data.loc[cleaned_data.margin_fin.isna(), 'margin_fin'] = cleaned_data['eqty_tot'] - cleaned_data['asst_current']\n",
    "    cleaned_data.loc[cleaned_data.roe.isna(), 'roe'] = cleaned_data['prof_operations'] / cleaned_data['eqty_tot']\n",
    "    \n",
    "    ## Filling missing data with 0s for HQ_City (categorical) and the fs categories that would be nan for being 0,\n",
    "    ## based on data definitions.\n",
    "    ## As well, Converting categorical columns to categorical type\n",
    "\n",
    "    cleaned_data['HQ_city'] = cleaned_data['HQ_city'].fillna(0).astype('category')\n",
    "    cleaned_data['legal_struct'] = cleaned_data['legal_struct'].astype('category')\n",
    "    cleaned_data['ateco_sector'] = cleaned_data['ateco_sector'].astype('category')\n",
    "\n",
    "    ## calculating inventory from definition\n",
    "    cleaned_data['inventory'] = cleaned_data['asst_current'] - cleaned_data['cash_and_equiv'] - cleaned_data['AR']\n",
    "    \n",
    "    ## building liquidity ratios\n",
    "    cleaned_data['current_ratio'] = cleaned_data['asst_current'] / cleaned_data['debt_st']\n",
    "    cleaned_data['quick_ratio'] = (cleaned_data['cash_and_equiv'] + cleaned_data['AR'] \n",
    "                                  -  cleaned_data['AP_st'])/ cleaned_data['debt_st']\n",
    "    cleaned_data['cash_ratio'] = cleaned_data['cash_and_equiv'] / cleaned_data['debt_st']\n",
    "    cleaned_data['cfo_ratio'] = cleaned_data['cf_operations'] / cleaned_data['debt_st']\n",
    "    cleaned_data['defensive_interval'] = 365 * (cleaned_data['cash_and_equiv'] + \n",
    "                                                cleaned_data['AR'] -  cleaned_data['AP_st']) / cleaned_data['COGS']\n",
    "    liquidity_ratio_cols = ['current_ratio', 'quick_ratio', 'cash_ratio', 'cfo_ratio', 'defensive_interval']                                            \n",
    "\n",
    "    ## building activity ratios\n",
    "    ## get rid of avg_days color \n",
    "    cleaned_data['receivables_turnover'] = cleaned_data['rev_operating'] / cleaned_data['AR']\n",
    "    cleaned_data['inventory_turnover'] = cleaned_data['COGS'] / cleaned_data['inventory']\n",
    "    cleaned_data['payables_turnover'] = (cleaned_data['COGS'] + cleaned_data['inventory']) / cleaned_data['AP_st']\n",
    "    cleaned_data['operating_cycle'] = ((365 / cleaned_data['receivables_turnover']) + \n",
    "                                       (365 / cleaned_data['inventory_turnover']))\n",
    "    cleaned_data['net_cash_cycle'] = cleaned_data['operating_cycle'] - (365 / cleaned_data['payables_turnover'])\n",
    "    cleaned_data['working_capital_turnover'] = cleaned_data['rev_operating'] / cleaned_data['wc_net']\n",
    "    activity_ratio_cols = ['receivables_turnover', 'inventory_turnover', 'payables_turnover', 'net_cash_cycle', \n",
    "                           'working_capital_turnover']\n",
    "\n",
    "    ## building solvency ratios\n",
    "    cleaned_data['debt_to_total_assets'] = (cleaned_data['debt_st'] + cleaned_data['debt_lt']) / cleaned_data['asst_tot']\n",
    "    cleaned_data['debt_to_equity'] = (cleaned_data['debt_st'] + cleaned_data['debt_lt']) / cleaned_data['eqty_tot']\n",
    "    cleaned_data['financial_leverage'] = cleaned_data['asst_tot'] / cleaned_data['eqty_tot']\n",
    "    cleaned_data['debt_service_coverage'] = cleaned_data['prof_operations'] / cleaned_data['debt_st']\n",
    "    cleaned_data['cfo_to_debt'] = cleaned_data['cf_operations'] / (cleaned_data['debt_st'] + cleaned_data['debt_lt'])\n",
    "    cleaned_data['cfo_to_operating_earnings'] = cleaned_data['cf_operations'] / cleaned_data['prof_operations']\n",
    "    solvency_ratio_cols =  ['debt_to_total_assets', 'debt_to_equity', 'financial_leverage', 'debt_service_coverage',\n",
    "                            'cfo_to_debt', 'cfo_to_operating_earnings']\n",
    "\n",
    "    ## building profitability ratios\n",
    "    cleaned_data['roic'] = (cleaned_data['prof_operations'] - \n",
    "                            cleaned_data['taxes']) / (cleaned_data['wc_net'] + cleaned_data['asst_intang_fixed'] +\n",
    "                                                      cleaned_data['asst_tang_fixed'] + cleaned_data['asst_fixed_fin'])\n",
    "    cleaned_data['operating_margin'] = cleaned_data['ebitda'] / cleaned_data['rev_operating']\n",
    "    cleaned_data['gross_profit_margin'] = (cleaned_data['rev_operating'] - \n",
    "                                           cleaned_data['COGS']) / cleaned_data['rev_operating']\n",
    "    cleaned_data['net_profit_margin_on_sales'] = cleaned_data['profit'] / cleaned_data['rev_operating']\n",
    "    cleaned_data['cash_roa'] = cleaned_data['cf_operations'] / cleaned_data['asst_tot']\n",
    "    profitability_ratio_cols = ['roe', 'roic', 'operating_margin', 'gross_profit_margin', \n",
    "                                'net_profit_margin_on_sales', 'cash_roa']\n",
    "\n",
    "    ## Assembling list of ratios\n",
    "    fs_ratio_cols = liquidity_ratio_cols + activity_ratio_cols + solvency_ratio_cols + profitability_ratio_cols\n",
    "\n",
    "    ## Replacing infinite values with NaNs\n",
    "    cleaned_data = cleaned_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    ## imputing missing values for fs_ratio_cols using (train data) industry means\n",
    "    if new:\n",
    "      ## creating dict of grouped means\n",
    "      preproc_params['fs_ratio_group_means'] = {}\n",
    "      ## iterating through ratio cols\n",
    "      for ratio_col in fs_ratio_cols:\n",
    "        ## creating series of grouped means for ratio cols\n",
    "        preproc_params['fs_ratio_group_means'][ratio_col] = cleaned_data.groupby('ateco_sector')[ratio_col].mean()\n",
    "        ## filling NAs in each group with the mean\n",
    "        for group in preproc_params['fs_ratio_group_means'][ratio_col].index:\n",
    "          cleaned_data.loc[(cleaned_data[ratio_col].isnull()) & (cleaned_data['ateco_sector'] == group), \n",
    "                           ratio_col] = preproc_params['fs_ratio_group_means'][ratio_col][group]\n",
    "    else:\n",
    "      ## iterating through ratio cols\n",
    "      for ratio_col in preproc_params['fs_ratio_group_means'].keys():\n",
    "        ## filling NAs in each group with the mean\n",
    "        for group in preproc_params['fs_ratio_group_means'][ratio_col].index:\n",
    "          cleaned_data.loc[(cleaned_data[ratio_col].isnull()) & (cleaned_data['ateco_sector'] == group), \n",
    "                           ratio_col] = preproc_params['fs_ratio_group_means'][ratio_col][group]\n",
    "\n",
    "    ## Doing Principal Component Analysis on each of the ratios to reduce dimentionality\n",
    "    liquidity_ratio_df = cleaned_data[liquidity_ratio_cols].copy()\n",
    "    activity_ratio_cols_df = cleaned_data[activity_ratio_cols].copy()\n",
    "    solvency_ratio_cols_df = cleaned_data[solvency_ratio_cols].copy()\n",
    "    profitability_ratio_cols_df = cleaned_data[profitability_ratio_cols].copy()\n",
    "\n",
    "    liquidity_ratio_pca, preproc_params = reduce_ratio_dimentionality(liquidity_ratio_df, \"liquidity\", new, preproc_params)\n",
    "    activity_ratio_pca, preproc_params = reduce_ratio_dimentionality(activity_ratio_cols_df, \"activity\", new, preproc_params)\n",
    "    solvency_ratio_pca, preproc_params = reduce_ratio_dimentionality(solvency_ratio_cols_df, \"solvency\", new, preproc_params)\n",
    "    profitability_ratio_pca, preproc_params = reduce_ratio_dimentionality(profitability_ratio_cols_df, \"profitability\", new, preproc_params)\n",
    "    cleaned_data['liquidity_ratio'] = liquidity_ratio_pca\n",
    "    cleaned_data['activity_ratio'] = activity_ratio_pca\n",
    "    cleaned_data['solvency_ratio'] = solvency_ratio_pca\n",
    "    cleaned_data['profitability_ratio'] = profitability_ratio_pca\n",
    "\n",
    "    ## Assembling list of reduced ratios\n",
    "    reduced_ratio_cols = ['liquidity_ratio', 'activity_ratio', 'solvency_ratio', 'profitability_ratio']\n",
    "    \n",
    "    ## integrating economic data\n",
    "    italy_econ_data = pd.read_csv(data_path + \"italy_economic_data.csv\")\n",
    "    italy_econ_data['Date'] = pd.to_datetime(italy_econ_data['Date'])\n",
    "    italy_econ_data = italy_econ_data.set_index('Date')\n",
    "    cleaned_data = pd.merge(cleaned_data, italy_econ_data,\n",
    "                            left_on='stmt_date', right_index=True)\n",
    "    italy_econ_cols = list(italy_econ_data.columns)\n",
    "    \n",
    "    #categorical variables\n",
    "    cat_cols = ['legal_struct', 'ateco_sector']\n",
    "    categorical_columns = []\n",
    "\n",
    "    for col in cat_cols:\n",
    "      dummies = pd.get_dummies(cleaned_data[col])\n",
    "      new_columns_names = [col+'_'+str(i) for i in dummies.columns]\n",
    "      categorical_columns += new_columns_names\n",
    "      dummies.columns = new_columns_names\n",
    "      cleaned_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "      cleaned_data = pd.concat([cleaned_data, dummies], axis=1)\n",
    "\n",
    "    if new:\n",
    "      ## building outcome variable using default_function\n",
    "      cleaned_data['defaulted_within_12_months'] = cleaned_data.apply(lambda x: default_function(x), axis=1)\n",
    "      del cleaned_data['def_date']\n",
    "\n",
    "      true_pd = cleaned_data['defaulted_within_12_months'].mean()\n",
    "      preproc_params['true_pd'] = true_pd\n",
    "\n",
    "    data_cols = [col for col in cleaned_data.columns if\n",
    "                 col in reduced_ratio_cols + italy_econ_cols + categorical_columns + ['defaulted_within_12_months']]\n",
    "    cleaned_data = cleaned_data[data_cols]\n",
    "    \n",
    "    num_features = reduced_ratio_cols + italy_econ_cols\n",
    "\n",
    "    ## Standardizing data\n",
    "    if new:\n",
    "      scaler = RobustScaler()\n",
    "      cleaned_data[num_features] = scaler.fit_transform(cleaned_data[num_features])\n",
    "      preproc_params['scaler'] = scaler\n",
    "    else:\n",
    "      scaler = preproc_params['scaler']\n",
    "      cleaned_data[num_features] = scaler.transform(cleaned_data[num_features])\n",
    "      \n",
    "    return cleaned_data, preproc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MSaP8U_AxGpO"
   },
   "outputs": [],
   "source": [
    "def create_calibrator(true_pd, sample_set_pd):\n",
    "  \n",
    "\n",
    "  '''\n",
    "  Function to create a calibrator:\n",
    "  Arguments:\n",
    "    true_pd = True probability of default\n",
    "    sample_set = sample set probability of default\n",
    "  '''\n",
    "  def calibrator(model_pd):\n",
    "\n",
    "    corrected_pd = ((true_pd)*(model_pd - model_pd*sample_set_pd))/(sample_set_pd - model_pd*sample_set_pd + model_pd*true_pd - sample_set_pd*true_pd)\n",
    "\n",
    "    return corrected_pd\n",
    "\n",
    "  return calibrator\n",
    "\n",
    "def estimator(cleaned_data, fitting_algo, calibrator, est_params = {'max_depth':60, 'max_leaf_nodes':1000, 'target_name':'defaulted_within_12_months'}):\n",
    "\n",
    "    model = fitting_algo(max_depth = est_params['max_depth'])\n",
    "\n",
    "    X = cleaned_data.loc[:, cleaned_data.columns!=est_params['target_name']]\n",
    "    y = cleaned_data[est_params['target_name']]\n",
    "    model.fit(X,y)\n",
    "\n",
    "    y_pred_proba = model.predict_proba(X)[:,1]\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba_calibrated = calibrator(y_pred_proba)\n",
    "\n",
    "\n",
    "    # print('Before Calibration:')\n",
    "    # draw_calibration_curve(y, y_pred_proba)\n",
    "    # print('After Calibration:')\n",
    "    # draw_calibration_curve(y, y_pred_proba_calibrated)\n",
    "\n",
    "    print('Area under ROC curve:', roc_auc_score(y, y_pred_proba))\n",
    "    fpr,tpr,threshs = roc_curve(y, y_pred_proba)\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.show()   \n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "    return(model)\n",
    "\n",
    "def predictor(new_df, model, calibrator):\n",
    "    # your code here\n",
    "    predictions = model.predict(new_df)\n",
    "    predictions_proba = model.predict_proba(new_df)[:,1]\n",
    "    calibrated_predictions_proba = np.array([calibrator(a) for a in predictions_proba])\n",
    "\n",
    "    return(predictions,calibrated_predictions_proba)\n",
    "\n",
    "def test_harness(df, preprocessor=preprocessor, estimator=estimator, predictor=predictor, data_path=None):     \n",
    "    new_df = df.copy()\n",
    "\n",
    "    if data_path is None:\n",
    "      print(\"Please give Data Path\")\n",
    "      return None\n",
    "\n",
    "    ## load saved model\n",
    "    with open(data_path + 'model.pkl' , 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    ## load save preprocessing parameters\n",
    "    with open(data_path + 'preproc_params.pkl', 'rb') as fid:\n",
    "        preproc_params = pickle.load(fid)\n",
    "    \n",
    "    ## making sure data is in chronological order\n",
    "    new_df = new_df.sort_values(preproc_params['year_col'])\n",
    "\n",
    "    sample_set_pd = preproc_params['sample_set_pd']\n",
    "    true_pd = preproc_params['true_pd']\n",
    "    \n",
    "    calibrator = create_calibrator(true_pd, sample_set_pd)\n",
    "    ## Preprocessing the test data\n",
    "    preproc_test_set, test_preproc_params = preprocessor(df, preproc_params, new=False, data_path=data_path)\n",
    "    ## Predicting the default and getting the probabilities\n",
    "    predictions, pred_proba = predictor(preproc_test_set, model, calibrator)\n",
    "    \n",
    "    return pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMyt2vZY0eAr",
    "outputId": "f5f50cdd-9d5f-40eb-825b-11e3192e2afa"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'valid.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-17f27e3c5ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## load the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpredicted_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'valid.csv'"
     ]
    }
   ],
   "source": [
    "## Kindly install \"imbalanced-learn\" package\n",
    "## Eg. conda install -c conda-forge imbalanced-learn\n",
    "\n",
    "## Path where all the data is stored\n",
    "data_path = \"./\"\n",
    "## load the test data\n",
    "test_data = pd.read_csv('valid.csv')\n",
    "\n",
    "predicted_probabilities = test_harness(test_data, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
